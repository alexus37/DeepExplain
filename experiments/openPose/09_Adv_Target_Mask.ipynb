{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Target mask for adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# imports\n",
    "import copy\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm import tqdm\n",
    "\n",
    "from loss_functions import final_stage_heat_loss, multi_stage_loss\n",
    "from plot_utils import gen_plot, plot_human_lines, plot_pose\n",
    "from tf_pose import common\n",
    "from tf_pose.common import CocoPart\n",
    "from tf_pose.estimator import PoseEstimator, TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path\n",
    "from utils import print_image_Stats, compare_poses, get_humans_as_lines\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger('TfPoseEstimatorRun').setLevel(logging.ERROR)\n",
    "logging.getLogger('DeepExplain').setLevel(logging.ERROR)\n",
    "logging.getLogger('TfPoseEstimator').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params 432, 368\n",
    "w, h = 432, 368\n",
    "image_path_source = '../data/images/dri_source_2.jpg'\n",
    "image_path_source_mask = '../data/images/dri_target_far_2_mask.jpg'\n",
    "image_path_target = '../data/images/dri_target_far_2.jpg'\n",
    "model = 'cmu'\n",
    "log_dir = '../logs/'\n",
    "resize_out_ratio = 2.0\n",
    "image_source = common.read_imgfile(image_path_source, w, h)\n",
    "image_target = common.read_imgfile(image_path_target, w, h)\n",
    "image_source_mask = common.read_imgfile(\n",
    "    image_path_source_mask, w, h, cv2.IMREAD_GRAYSCALE) > 0\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\n",
    "    log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_source = TfPoseEstimator(get_graph_path(\n",
    "    model), target_size=(w, h), trt_bool=False)\n",
    "e_target = TfPoseEstimator(get_graph_path(\n",
    "    model), target_size=(w, h), trt_bool=False)\n",
    "upsample_size = [int(e_target.target_size[1] / 8 * resize_out_ratio),\n",
    "                 int(e_target.target_size[0] / 8 * resize_out_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_source = e_source.inference(image_source, resize_to_default=(\n",
    "    w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "plot_pose(image_source, human_source, e_source.heatMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_target = e_target.inference(image_target, resize_to_default=(\n",
    "    w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "plot_pose(image_target, human_target, e_target.heatMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lines = get_humans_as_lines(human_source, 400, 450)\n",
    "target_lines = get_humans_as_lines(human_target, 400, 450)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "plot_human_lines(source_lines, ax, color='r', linestyle='-', label='source')\n",
    "plot_human_lines(target_lines, ax, color='g', linestyle='-', label='target')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], color='r', label='source'),\n",
    "                   Line2D([0], [0], color='g', label='target'),\n",
    "                   Line2D([0], [0], color='b', label='adverserial')]\n",
    "ax.legend(handles=legend_elements, loc='best',  prop={'size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get all outputs as np matrix\n",
    "target_np, target_np_heat, target_np_paf, target_np_peaks = e_target.persistent_sess.run(\n",
    "    [e_target.tensor_output, e_target.tensor_heatMat_up,\n",
    "        e_target.tensor_pafMat_up, e_target.tensor_peaks],\n",
    "    feed_dict={\n",
    "        e_target.tensor_image: [image_target],\n",
    "        e_target.upsample_size: upsample_size\n",
    "    }\n",
    ")\n",
    "\n",
    "source_np, source_np_heat, source_np_paf, source_np_peaks = e_source.persistent_sess.run(\n",
    "    [e_source.tensor_output, e_source.tensor_heatMat_up,\n",
    "        e_source.tensor_pafMat_up, e_source.tensor_peaks],\n",
    "    feed_dict={\n",
    "        e_source.tensor_image: [image_source],\n",
    "        e_source.upsample_size: upsample_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_np.shape)\n",
    "print(target_np_heat.shape)\n",
    "print(target_np_paf.shape)\n",
    "print(target_np_peaks.shape)\n",
    "\n",
    "# for i in range(52):\n",
    "#     print(np.sum(np.abs(target_np[:, :, :, i] - source_np[:, :, :, i])))\n",
    "\n",
    "\n",
    "human_source = PoseEstimator.estimate_paf(\n",
    "    source_np_peaks[0], source_np_heat[0], source_np_paf[0])\n",
    "human_target = PoseEstimator.estimate_paf(\n",
    "    target_np_peaks[0], target_np_heat[0], target_np_paf[0])\n",
    "\n",
    "\n",
    "source_lines = get_humans_as_lines(human_source, 400, 450)\n",
    "target_lines = get_humans_as_lines(human_target, 400, 450)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "plot_human_lines(source_lines, ax, color='r', linestyle='-', label='source')\n",
    "plot_human_lines(target_lines, ax, color='g', linestyle='-', label='target')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], color='r', label='source'),\n",
    "                   Line2D([0], [0], color='g', label='target')]\n",
    "ax.legend(handles=legend_elements, loc='best',  prop={'size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 5000\n",
    "STEPS = 100\n",
    "STAGE_INDEX = 6  # range from 2 to 6\n",
    "ONLY_HEAT_MAP_AS_LOSS = False\n",
    "\n",
    "\n",
    "if not \"adv_image\" in vars():\n",
    "    adv_image = copy.deepcopy(image_source)\n",
    "target_heat, grad_func, loss_func = [None, None, None]\n",
    "\n",
    "human_source = PoseEstimator.estimate_paf(\n",
    "    source_np_peaks[0], source_np_heat[0], source_np_paf[0])\n",
    "human_target = PoseEstimator.estimate_paf(\n",
    "    target_np_peaks[0], target_np_heat[0], target_np_paf[0])\n",
    "\n",
    "\n",
    "if ONLY_HEAT_MAP_AS_LOSS:\n",
    "    target_heat = tf.compat.v1.placeholder(\n",
    "        tf.float32, shape=(None, None, None, 19))\n",
    "    grad_func, loss_func = final_stage_heat_loss(e_source, target_heat)\n",
    "else:\n",
    "\n",
    "    target_heat = tf.compat.v1.placeholder(\n",
    "        tf.float32, shape=(None, None, None, 19))\n",
    "    target_paf = tf.compat.v1.placeholder(\n",
    "        tf.float32, shape=(None, None, None, 38))\n",
    "    grad_func, loss_func = multi_stage_loss(\n",
    "        e_source, target_heat, target_paf, STAGE_INDEX)\n",
    "\n",
    "for i in tqdm(range(STEPS)):\n",
    "    # compute gradient\n",
    "    gradient, gn_summ = e_source.persistent_sess.run(\n",
    "        [grad_func, loss_func],\n",
    "        feed_dict={\n",
    "            target_heat: target_np[:, :, :, CocoPart.RWrist.value] if ONLY_HEAT_MAP_AS_LOSS else target_np[:, :, :, :19],\n",
    "            target_paf: target_np[:, :, :, 19:],\n",
    "            e_source.tensor_image: [adv_image],\n",
    "            e_source.upsample_size: upsample_size\n",
    "        }\n",
    "    )\n",
    "    summary_writer.add_summary(gn_summ, i)\n",
    "    if i % 20 == 0:\n",
    "        human_adv = e_source.inference(adv_image, resize_to_default=(\n",
    "            w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "        # Prepare the plot\n",
    "        plot_buf = gen_plot(human_source, human_target, human_adv)\n",
    "\n",
    "        # Convert PNG buffer to TF image\n",
    "        tf_image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "\n",
    "        # Add the batch dimension\n",
    "        tf_image = tf.expand_dims(tf_image, 0)\n",
    "\n",
    "        # Add image summary\n",
    "        summary_op = tf.summary.image(f\"poses_step{i}\", tf_image)\n",
    "        summary_img = e_source.persistent_sess.run(summary_op)\n",
    "        # Write summary\n",
    "\n",
    "        summary_writer.add_summary(summary_img)\n",
    "        summary_writer.flush()\n",
    "\n",
    "    ### ================= UPDATE STEP ================= ###\n",
    "    #adv_image = adv_image - EPS * np.sign(gradient[0])\n",
    "    scaled_gradient = image_source_mask[..., None] * (EPS * gradient[0])\n",
    "    adv_image = adv_image - scaled_gradient\n",
    "    adv_image = np.clip(adv_image, 0, 255)\n",
    "\n",
    "summary_writer.flush()\n",
    "\n",
    "human_adv = e_source.inference(adv_image, resize_to_default=(\n",
    "    w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "\n",
    "print_image_Stats(image_source)\n",
    "print_image_Stats(adv_image)\n",
    "total_diff, most_moved_part = compare_poses(human_source[0], human_adv[0])\n",
    "print(\n",
    "    f'The total diff is {total_diff} and the part moved the most is the {most_moved_part}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 10000\n",
    "print(gradient.shape)\n",
    "print_image_Stats(gradient[0])\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(gradient[0] * SCALE)\n",
    "\n",
    "\n",
    "grad_len = np.zeros((gradient[0].shape[:2]))\n",
    "for y in range(gradient[0].shape[0]):\n",
    "    for x in range(gradient[0].shape[1]):\n",
    "        cur_vec = gradient[0, y, x]\n",
    "        grad_len[y, x] = np.linalg.norm(cur_vec)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "heat_image = ax.imshow(grad_len, cmap='hot')\n",
    "fig.colorbar(heat_image, ax=ax, shrink=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lines = get_humans_as_lines(human_source, 400, 450)\n",
    "target_lines = get_humans_as_lines(human_target, 400, 450)\n",
    "adv_lines = get_humans_as_lines(human_adv, 400, 450)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "plot_human_lines(source_lines, ax, color='r', linestyle='-', label='source')\n",
    "plot_human_lines(target_lines, ax, color='g', linestyle='-', label='target')\n",
    "plot_human_lines(adv_lines, ax, color='b', linestyle='--', label='adv')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], color='r', label='source'),\n",
    "                   Line2D([0], [0], color='g', label='target'),\n",
    "                   Line2D([0], [0], color='b', label='adverserial')]\n",
    "ax.legend(handles=legend_elements, loc='best',  prop={'size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pose(image_source, human_adv, e_source.heatMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 25))\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 1)\n",
    "ax.imshow(cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Original')\n",
    "\n",
    "adv_image_cliped = np.clip(adv_image, 0, 255) / 255\n",
    "ax = fig.add_subplot(1, 4, 2)\n",
    "ax.imshow(cv2.cvtColor(adv_image_cliped, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Adversarial image')\n",
    "\n",
    "\n",
    "image_source_result = TfPoseEstimator.draw_humans(\n",
    "    image_source, human_source, imgcopy=True)\n",
    "image_source_result = TfPoseEstimator.draw_humans(\n",
    "    image_source_result, human_adv, imgcopy=True)\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 3)\n",
    "ax.set_title('Poses compared')\n",
    "ax.imshow(cv2.cvtColor(image_source_result, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 4)\n",
    "ax.set_title('Adversarial Noise')\n",
    "noise = image_source - adv_image\n",
    "ax.imshow(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 25))\n",
    "adv_image_cliped = np.clip(adv_image, 0, 255) / 255\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(cv2.cvtColor(adv_image_cliped, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title('Adversarial image')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:masterThesis] *",
   "language": "python",
   "name": "conda-env-masterThesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
