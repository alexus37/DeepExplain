{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import enoki as ek\n",
    "import mitsuba\n",
    "mitsuba.set_variant('gpu_autodiff_rgb')\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from mitsuba.python.autodiff import render, render_torch, write_bitmap\n",
    "from mitsuba.python.util import traverse\n",
    "from mitsuba.core.xml import load_file\n",
    "from mitsuba.core import Thread, Vector3f,LogLevel\n",
    "\n",
    "\n",
    "from torch_openpose.body import Body\n",
    "from torch_openpose import util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def wandb_init(name):\n",
    "    run = wandb.init(\n",
    "        project = \"mts_mutliple_images\", \n",
    "        reinit = True,\n",
    "        name = name,\n",
    "        config={\n",
    "            \"epochs\": 5,\n",
    "            \"batch_size\": 4,\n",
    "            \"noise_width\": 200,\n",
    "            \"noise_height\": 400,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return run \n",
    "\n",
    "def get_scenes_filenames(directory):\n",
    "    scenes = []\n",
    "    for file in tqdm(glob.glob(f\"{directory}/*.xml\")):\n",
    "        scenes.append(file)\n",
    "        \n",
    "    # shuffel scenes due to naming \n",
    "    random.seed(42)\n",
    "    random.shuffle(scenes)\n",
    "    \n",
    "    return scenes\n",
    "\n",
    "def get_candidate_from_tensor(rendering_torch, body_estimation):\n",
    "    PERMUTE_BGR = [2, 1, 0]\n",
    "     # make torch RGB to BGR and permute and stack\n",
    "    rendering_torch_BGR = rendering_torch[:, :, PERMUTE_BGR]\n",
    "    rendering_torch_input = torch.stack([rendering_torch_BGR.permute((2, 0, 1))  - 0.5])\n",
    "    \n",
    "    # Render a reference image (no derivatives used yet)\n",
    "    heatmap_avg, paf_avg = body_estimation.compute_heatmap_paf_avg(rendering_torch_input, ORIG_SHAPE)\n",
    "    candidate, subset = body_estimation.get_pose(heatmap_avg, paf_avg, ORIG_SHAPE)\n",
    "    rendering_torch_np =  rendering_torch[:, :, PERMUTE_BGR].detach().cpu().numpy()\n",
    "    canvas = copy.deepcopy(rendering_torch_np)\n",
    "    canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "    \n",
    "    return canvas, candidate, subset\n",
    "\n",
    "def get_image_and_score(scene_path, params_torch, body_estimation, diff_parameter, spp=8):\n",
    "    thread = Thread.thread()\n",
    "    thread.file_resolver().append(test_dir)\n",
    "    logger = thread.logger()\n",
    "    logger.set_log_level(LogLevel.Warn)\n",
    "    \n",
    "    # load scene\n",
    "    scene = load_file(scene_path)\n",
    "    params = traverse(scene)\n",
    "    rendering_torch = render_torch(scene, spp=spp)\n",
    "    canvas_orig, candidate_orig, subset_orig = get_candidate_from_tensor(rendering_torch, body_estimation)\n",
    "   \n",
    "    \n",
    "    # update the params with the new noise value\n",
    "    params[diff_parameter] = params_torch[diff_parameter].detach().cpu().numpy()\n",
    "    params.update()\n",
    "    \n",
    "    rendering_torch = render_torch(scene, params=params, unbiased=True, spp=spp, **params_torch)\n",
    "    canvas_noise, candidate_noise, subset_noise = get_candidate_from_tensor(rendering_torch, body_estimation)\n",
    "    \n",
    "    metric = pose_loss_single_human(subset_noise, subset_orig)\n",
    "    return canvas_orig[:, :, [2, 1, 0]], canvas_noise[:, :, [2, 1, 0]], metric\n",
    "\n",
    "\n",
    "def pose_loss_single_human(newHuman, oldHuman):\n",
    "    if len(oldHuman) == 0 or len(newHuman) == 0:\n",
    "        return 0\n",
    "    new_detected = 0\n",
    "    old_detected = 0\n",
    "    for part in range(18):\n",
    "        if newHuman[0][part] != -1:\n",
    "            new_detected += 1\n",
    "        if oldHuman[0][part] != -1:\n",
    "            old_detected += 1\n",
    "    return new_detected / old_detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:00<00:00, 3355443.20it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 866591.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#CONSTS\n",
    "STRIDE = 8\n",
    "PAD_VALUE = 128\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 432, 368\n",
    "ORIG_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
    "snapshot_name = f'../snapshots/mts_noise.npy'\n",
    "train_dir = '../data/allTransforms/train'\n",
    "test_dir = '../data/allTransforms/test'\n",
    "\n",
    "train_scenes = get_scenes_filenames(train_dir)\n",
    "test_scenes = get_scenes_filenames(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "240000\n"
     ]
    }
   ],
   "source": [
    "body_estimation = Body('/home/ax/data/programs/pytorch-openpose/model/body_pose_model.pth', True)\n",
    "# set defaults\n",
    "body_estimation.imageToTest_padded_shape = ORIG_SHAPE\n",
    "body_estimation.pad = [0, 0, 0, 0]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "thread = Thread.thread()\n",
    "thread.file_resolver().append('pose_scene')\n",
    "logger = thread.logger()\n",
    "logger.set_log_level(LogLevel.Warn)\n",
    "\n",
    "scene = load_file('pose_scene/scene.xml')\n",
    "\n",
    "# Find differentiable scene parameters\n",
    "params = traverse(scene)\n",
    "print(len(params['rect.bsdf.reflectance.data']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nrendering_torch = render_torch(scene, spp=32)\\n\\n# make torch RGB to BGR\\nPERMUTE_BGR = [2, 1, 0]\\nrendering_torch_BGR = rendering_torch[:, :, PERMUTE_BGR]\\n# permute and stack\\nrendering_torch_input = torch.stack([rendering_torch_BGR.permute((2, 0, 1))  - 0.5])\\n\\ncanvas = rendering_torch_BGR.cpu().detach().numpy() \\nfig = plt.figure(figsize=(10, 10))\\nax = fig.add_subplot(1, 1, 1)\\nax.imshow(canvas[:, :, [2, 1, 0]])\\nax.axis('off')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render a reference image (no derivatives used yet)\n",
    "\"\"\"\n",
    "rendering_torch = render_torch(scene, spp=32)\n",
    "\n",
    "# make torch RGB to BGR\n",
    "PERMUTE_BGR = [2, 1, 0]\n",
    "rendering_torch_BGR = rendering_torch[:, :, PERMUTE_BGR]\n",
    "# permute and stack\n",
    "rendering_torch_input = torch.stack([rendering_torch_BGR.permute((2, 0, 1))  - 0.5])\n",
    "\n",
    "canvas = rendering_torch_BGR.cpu().detach().numpy() \n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(canvas[:, :, [2, 1, 0]])\n",
    "ax.axis('off')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/alexus/mts_mutliple_images\" target=\"_blank\">https://app.wandb.ai/alexus/mts_mutliple_images</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/alexus/mts_mutliple_images/runs/398mci2u\" target=\"_blank\">https://app.wandb.ai/alexus/mts_mutliple_images/runs/398mci2u</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [13:28<00:00,  1.80s/it]\n",
      "  0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [12:44<00:00,  1.70s/it]\n",
      "  0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 224/450 [06:23<06:27,  1.71s/it]"
     ]
    }
   ],
   "source": [
    "run = wandb_init('multipleImage')\n",
    "noise_resolution  = np.array([200, 400])\n",
    "steps = len(train_scenes) // run.config.batch_size\n",
    "\n",
    "diff_parameter = 'rect.bsdf.reflectance.data'\n",
    "\n",
    "# Which parameters should be exposed to the PyTorch optimizer?\n",
    "params.keep([diff_parameter])\n",
    "params.update()\n",
    "params_torch = params.torch() # main pytorch params\n",
    "\n",
    "# Construct a PyTorch Adam optimizer that will adjust 'params_torch'\n",
    "opt = torch.optim.Adam(params_torch.values(), lr=.2)\n",
    "objective = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# target\n",
    "zero_heatmap = torch.zeros((run.config.batch_size, 19, 46, 54), device=device)\n",
    "zero_paf = torch.zeros((run.config.batch_size, 38, 46, 54), device=device)\n",
    "\n",
    "myThread = Thread.thread()\n",
    "myThread.file_resolver().append(train_dir)\n",
    "logger = myThread.logger()\n",
    "logger.set_log_level(LogLevel.Warn)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(run.config.epochs):\n",
    "    print(f\"run {epoch + 1}/{run.config.epochs}\")\n",
    "    for step in tqdm(range(steps)):\n",
    "        if step % 50 ==  0:\n",
    "            write_bitmap('output/universal_noise_%03i.png' % step, params_torch[diff_parameter], noise_resolution)\n",
    "            np.save(snapshot_name, params_torch[diff_parameter].detach().cpu().numpy())\n",
    "        opt.zero_grad()\n",
    "        body_estimation.model.zero_grad()\n",
    "\n",
    "        # list of all rendering from this batch\n",
    "        rendering_batch = []\n",
    "\n",
    "        for i in range(step * run.config.batch_size, (step * run.config.batch_size) + run.config.batch_size):\n",
    "            scene = load_file(train_scenes[i])\n",
    "            params = traverse(scene)\n",
    "            params.keep([diff_parameter])\n",
    "\n",
    "            # update the diffparamter\n",
    "            params[diff_parameter] = params_torch[diff_parameter].detach().cpu().numpy()\n",
    "\n",
    "            params.update()\n",
    "            # render image\n",
    "            # maybe add malloc_trim=True if memory issues\n",
    "            cur_rednering_torch = render_torch(scene, params=params, unbiased=True, spp=8, **params_torch)\n",
    "\n",
    "            # transform from rgb to bgr\n",
    "            PERMUTE_BGR = [2, 1, 0]\n",
    "            rendering_torch_BGR = cur_rednering_torch[:, :, PERMUTE_BGR]\n",
    "            rendering_batch.append(rendering_torch_BGR.permute((2, 0, 1))  - 0.5)\n",
    "\n",
    "        # permute channels to pytorch order and stack\n",
    "        rendering_torch_input = torch.stack(rendering_batch)\n",
    "\n",
    "        # compute the avg heatmap and paf map\n",
    "        paf, heatmap = body_estimation.model(rendering_torch_input)\n",
    "\n",
    "\n",
    "        ob_val_heat = objective(heatmap, zero_heatmap)\n",
    "        ob_val_paf = objective(paf, zero_paf)\n",
    "        ob_val = ob_val_heat + ob_val_paf\n",
    "\n",
    "        # calc gradient\n",
    "        ob_val.backward()\n",
    "\n",
    "        # take a step in gradient direction\n",
    "        opt.step()\n",
    "\n",
    "        # log the loss\n",
    "        wandb.log({'loss': ob_val.item()}, step=global_step)\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print image\n",
    "noise_resolution  = np.array([200, 400])\n",
    "canvas = params_torch[diff_parameter].detach().cpu().numpy()\n",
    "canvas = canvas.reshape(noise_resolution[1], noise_resolution[0], -1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(canvas[:, :, [2, 1, 0]])\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas_orig_list = []\n",
    "canvas_noise_list = []\n",
    "metric_list = []\n",
    "\n",
    "for test_scene in tqdm(test_scenes):\n",
    "    canvas_orig, canvas_noise, metric = get_image_and_score(test_scene, params_torch, body_estimation, diff_parameter)\n",
    "    canvas_orig_list.append(canvas_orig)\n",
    "    canvas_noise_list.append(canvas_noise)\n",
    "    metric_list.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg metric: {np.mean(metric_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cnt = 16\n",
    "for i in range(plot_cnt):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(canvas_orig_list[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Original')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(canvas_noise_list[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Noise {metric_list[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
